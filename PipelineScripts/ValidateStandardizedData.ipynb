{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import math\n",
    "from dateutil.parser import parse\n",
    "from geopandas import GeoDataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import json\n",
    "import ast\n",
    "import sys\n",
    "import pickle\n",
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "os.chdir(\"/Users/sergiocamelo/Dropbox/Sergio-Joann/\")\n",
    "\n",
    "sys.path.insert(0, 'Code/VRPEngine/pyCode')\n",
    "sys.path.insert(0, 'Code/VRPEngine/C++Engine')\n",
    "sys.path.insert(0, 'Code/VRPEngine/pyCode/tsp')\n",
    "f = open('StandardizedData/report.txt', 'w+')\n",
    "\n",
    "import solver as solver\n",
    "import distances as distances\n",
    "import VRPClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = '/Users/sergiocamelo/Dropbox/Sergio-Joann/StandardizedData/'\n",
    "season = \"high\"\n",
    "period = 3*28 # Use three month period\n",
    "days_analysis = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_volume = season + '_volume'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "dim_farmers = pd.read_csv(data_folder+'dim_farmers.csv')\n",
    "dim_middlemen = pd.read_csv(data_folder+'dim_middlemen.csv')\n",
    "dim_mills = pd.read_csv(data_folder+'dim_mills.csv')\n",
    "harvest_frequency_mapping = pd.read_csv(data_folder+'harvest_frequency_mapping.csv')\n",
    "print(\"Number of plantations: %d\" % (len(dim_farmers)),file=f)\n",
    "print(\"Number of middlemen: %d\" % (len(dim_middlemen)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a unique identifier for the farmer plot\n",
    "dim_farmers['plot_id'] = [dim_farmers['farmer_id'][i] + '-'+str(dim_farmers['plot_number'][i]) for i in range(len(dim_farmers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate middleman capacity\n",
    "dim_middlemen['trucks_dict']  = dim_middlemen['trucks'].map(lambda d:ast.literal_eval(d))\n",
    "dim_middlemen['capacity'] = dim_middlemen['trucks_dict'].map(lambda d:np.sum([int(t)*d[t] for t in d.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join farmers and middlemen data\n",
    "# Change names of columns\n",
    "dim_farmers = dim_farmers.rename(index=str, columns={\"latitude\": \"latitude_farmer\", \"longitude\": \"longitude_farmer\"})\n",
    "dim_middlemen = dim_middlemen.rename(index=str, columns={\"latitude\": \"latitude_middleman\", \"longitude\": \"longitude_middleman\"})\n",
    "dim_mills = dim_mills.rename(index=str, columns={\"latitude\": \"latitude_mill\", \"longitude\": \"longitude_mill\"})\n",
    "\n",
    "result = pd.merge(dim_farmers, dim_middlemen, on=['cluster_id'], how='inner')\n",
    "print(\"Number of plantations with middleman: %d\" % (len(result)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if there are any duplicates\n",
    "result[result.duplicated(subset=['farmer_id','plot_number'], keep=False)].to_csv(data_folder+'data_cleaning_results/duplicates.csv')\n",
    "print(\"Total of duplicates: %d\" % (len(result[result.duplicated(subset=['farmer_id','plot_number'], keep=False)])),file=f)\n",
    "print (\"Duplicates saved in data_cleaning_results/duplicates.csv\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use only data with lat_lon and with productions\n",
    "df_full = result[np.logical_and(pd.notnull(result['longitude_farmer']),pd.notnull(result['latitude_farmer']))].copy()\n",
    "print(\"Number of plantations with latlon: %d\" % (len(df_full)),file=f)\n",
    "# Use data with productions\n",
    "df_full = df_full[df_full[season+'_rate']!=0].copy()\n",
    "df_full = df_full[pd.notnull(df_full[season+'_rate'])].copy()\n",
    "df_full = df_full[df_full[season+'_volume']!=0].copy()\n",
    "print(\"Number of plantations that produce during the season : %d\" % (len(df_full)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map number of days\n",
    "harvest_frequency_mapping_dict = {row[0]:row[1] for i,row in harvest_frequency_mapping.iterrows()}\n",
    "df_full['rate'] = df_full[season+'_rate'].map(harvest_frequency_mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Has pickup date \n",
    "df_full = df_full[pd.notnull(df_full['date_last_sold'])].copy()\n",
    "print(\"Number of plantations with last date: %d\" % (len(df_full)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate days of pickup\n",
    "ref_day = datetime.datetime.strptime('1/1/2000', \"%m/%d/%Y\")\n",
    "days = np.array([(parse(v)-ref_day).days for v in df_full['date_last_sold'].values])\n",
    "df_full['day_mod'] = days%period\n",
    "def calculate_pickup_days(row):\n",
    "    d = row['day_mod']\n",
    "    freq = row[season+'_rate']\n",
    "    l = []\n",
    "    for i in range(int(period/freq)):\n",
    "        l.append((d + i * freq)%period)\n",
    "    return l\n",
    "df_full['pickup_days'] = df_full.apply(calculate_pickup_days, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explote data\n",
    "clusters = np.unique(df_full['cluster_id'])\n",
    "df_exploted = pd.merge(df_full,df_full.pickup_days.apply(pd.Series).stack().reset_index(level=1, drop=True).to_frame('pickup'),left_index=True, right_index=True)\n",
    "for c in clusters:\n",
    "    print(\"Number of plantations of cluster %d: %d\" % (c,len(df_full[df_full['cluster_id'] == c].copy())),file=f)\n",
    "df_clusters = df_exploted[(np.array([c in clusters for c in df_exploted['cluster_id']])) & (df_exploted['pickup'] < days_analysis)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print middlemen data in the results folder\n",
    "dim_middlemen[[dim_middlemen['cluster_id'][i] in clusters for i in range(len(dim_middlemen))]][['cluster_id','trucks','mills','capacity']].to_csv(data_folder+\"tables_for_report/middlemen_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate total capacity\n",
    "dict_comparisons={}\n",
    "for c in clusters:\n",
    "    dict_comparisons[c] = {}\n",
    "    dict_comparisons[c]['capacity'] = dim_middlemen[dim_middlemen.cluster_id == c]['capacity'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Round producing quantities to the decimal up\n",
    "df_clusters[season + '_volume'] = np.ceil(df_clusters[season + '_volume'] * 10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   farmer_id  high_volume  overload\n",
      "cluster_id pickup                                  \n",
      "35         9.0             6         17.8       6.8\n",
      "           10.0            6         22.8      11.8\n",
      "108        4.0            10         18.3       0.3\n",
      "591        9.0             6         11.7       2.7\n"
     ]
    }
   ],
   "source": [
    "# Number of plantations picked up each day and quantities picked up each day\n",
    "# Calculate the number of days\n",
    "agg_quant = df_clusters.groupby(['cluster_id','pickup']).agg({'farmer_id':'count', season+'_volume': 'sum'})\n",
    "agg_quant['overload']=agg_quant[season+'_volume']-agg_quant.apply(lambda r:dict_comparisons[r.name[0]]['capacity'],1)\n",
    "outliers = (agg_quant[agg_quant['overload']>0])\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a report of inconsistent data\n",
    "report = []\n",
    "for index,row in outliers.iterrows():\n",
    "    report.append({'cluster':index[0],\n",
    "                   'farmers':row['farmer_id'],\n",
    "                         season+'_volume':row[season+'_volume'],\n",
    "                         'capacity':row[season+'_volume']-row['overload'],\n",
    "                        'farmer_id-plot':[r['farmer_id']+'-'+str(r['plot_number']) for j,r in df_clusters.iterrows() if (r['cluster_id']==index[0] and r['pickup']==index[1])]})\n",
    "print(\"Found %d trucks carrying more than their capacity\" % len(outliers),file=f)\n",
    "pd.DataFrame(report).to_csv(data_folder+'data_cleaning_results/overcapacity.csv')\n",
    "print(\"Report saved in data_cleaning_results/overcapacity.csv\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clusters_c = df_clusters.copy()\n",
    "moved_farmers = 0\n",
    "# Modify inconsistent data, changing the pickup days randomly\n",
    "for index,row in outliers.iterrows():\n",
    "    success = False\n",
    "    df_cluster_outlier = df_clusters_c[df_clusters_c['cluster_id']==index[0]]\n",
    "    df_cluster_pickup_outlier = df_clusters_c[(df_clusters_c['cluster_id']==index[0])&(df_clusters_c['pickup']==index[1])]\n",
    "    cluster_capacity = dict_comparisons[index[0]]['capacity']\n",
    "    overload = np.sum(df_cluster_pickup_outlier[season+'_volume']) - cluster_capacity\n",
    "    # Extract rows to move\n",
    "    for index_to_replace, row_to_replace in df_cluster_pickup_outlier.iterrows():\n",
    "        row_capacity = row_to_replace[season+'_volume']\n",
    "        # Calculate the overloads of all pickup days\n",
    "        df_cluster_outlier = df_clusters_c[df_clusters_c['cluster_id']==index[0]]\n",
    "        agg_quant = df_cluster_outlier.groupby(['pickup']).agg({'farmer_id':'count', season+'_volume': 'sum'})\n",
    "        agg_quant['overload']=agg_quant[season+'_volume']-cluster_capacity\n",
    "        for index_to_replace_for,row_to_replace_for in agg_quant.iterrows():\n",
    "            if (row_to_replace_for['overload'] + row_capacity) <= 0:\n",
    "                df_clusters_c.loc[index_to_replace,'pickup'] = index_to_replace_for\n",
    "                moved_farmers +=1\n",
    "                break\n",
    "        df_cluster_pickup_outlier = df_clusters_c[(df_clusters_c['cluster_id']==index[0])&(df_clusters_c['pickup']==index[1])]\n",
    "        overload = np.sum(df_cluster_pickup_outlier[season+'_volume']) - cluster_capacity\n",
    "        if (overload <= 0):\n",
    "            success = True\n",
    "            break\n",
    "    if not success:\n",
    "        print(\"Not possible to make data consistent\",file=f)\n",
    "print(\"Fixed consistency with moving schedules of %d farmers\" % moved_farmers,file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parses data into numeric JSON format that can be read by C++\n",
    "def json_parser(H,N,H_p,N_p,capacities,quantities,mapping, distances, n_trucks):\n",
    "    N_ = list(range(len(N)))\n",
    "    H_ = list(range(len(N),len(N)+len(H)))\n",
    "    capacities_ = [int(capacities[h]*10) for h in H]\n",
    "    quantities_ = [int(quantities[n]*10) for n in N]\n",
    "    n_trucks_ = [int(n_trucks[h]) for h in H]\n",
    "    file_dict = {\"H\":H_,\n",
    "                \"N\":N_,\n",
    "                \"H_p\":H_p.tolist(),\n",
    "                \"N_p\":N_p.tolist(),\n",
    "                \"capacities\": capacities_,\n",
    "                \"quantities\": quantities_,\n",
    "                \"mapping\": mapping,\n",
    "                \"distances\": distances.tolist(),\n",
    "                \"n_trucks\": n_trucks_,\n",
    "                }\n",
    "    return file_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Create trucks dataset (Only run once)\n",
    "# truck_dicts = []\n",
    "# j = 0\n",
    "# for i,r in dim_middlemen.iterrows():\n",
    "#     truck_dic = ast.literal_eval(r['trucks'])\n",
    "#     for capacity in truck_dic.keys():\n",
    "#         for i in range(truck_dic[capacity]):\n",
    "#             truck_dicts.append({\n",
    "#                     \"cluster_id\":r['cluster_id'],\n",
    "#                     \"truck_id\":\"t_\"+str(j),\n",
    "#                     \"capacity\":int(capacity)\n",
    "#                 })\n",
    "#             j += 1\n",
    "# pd.DataFrame(truck_dicts).to_csv(data_folder+\"dim_trucks.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the trucks dataset\n",
    "dim_trucks = pd.read_csv(data_folder+'dim_trucks.csv')\n",
    "# Change names of datasets\n",
    "df_clusters_original = df_clusters.copy()\n",
    "df_clusters = df_clusters_c.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataset with code to position\n",
    "positions_dict = {}\n",
    "for i, row in dim_mills.iterrows():\n",
    "    positions_dict[row[\"code\"]] = (row['latitude_mill'], row['longitude_mill'])\n",
    "for i, row in dim_farmers.iterrows():\n",
    "    positions_dict[row[\"farmer_id\"]+'-'+str(row[\"plot_number\"])] = (row['latitude_farmer'], row['longitude_farmer'])\n",
    "for i, row in dim_middlemen.iterrows():\n",
    "    positions_dict[row[\"cluster_id\"]] = (row['latitude_middleman'], row['longitude_middleman'])\n",
    "for i, row in dim_trucks.iterrows():\n",
    "    positions_dict[row[\"truck_id\"]] = positions_dict[float(row[\"cluster_id\"])]\n",
    "unique_id_to_latlon = pd.DataFrame(positions_dict).transpose()\n",
    "unique_id_to_latlon.columns = ['latitude', 'longitude']\n",
    "unique_id_to_latlon.to_csv(data_folder+\"unique_id_to_latlon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if farmers sell more than the trucks capacity\n",
    "inconsistencies = 0\n",
    "for i, row in df_clusters.iterrows():\n",
    "    max_capacity = np.max([int(k) for k in row['trucks_dict'].keys()])\n",
    "    if row[season+'_volume']>max_capacity:\n",
    "        df_clusters.loc[i,season+'_volume'] = max_capacity\n",
    "        inconsistencies += 1\n",
    "print(\"A total of %d instances where farmer produced more than truck were found and replaced\" % inconsistencies,file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset of daily production of middlemen\n",
    "df_clusters.groupby(['cluster_id','pickup'])[['high_volume','low_volume']].sum().to_csv(data_folder+\"middlemen_daily_production.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VRPClass = reload(VRPClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create daily instances\n",
    "bash_path = data_folder+\"instances/bash_daily.sh\"\n",
    "open(bash_path, 'w').close()\n",
    "for c in clusters:\n",
    "    for d in range(days_analysis):\n",
    "        # Extract mill coordinates\n",
    "        M_p = np.array(dim_mills[dim_mills['code'] == 'SKIP'][[\"latitude_mill\",\"longitude_mill\"]].values)\n",
    "        trucks = dim_trucks[dim_trucks['cluster_id']==c]['truck_id'].values\n",
    "        k = len(trucks)\n",
    "        H = ['h_'+str(i) for i in range(k)]\n",
    "        H_p = np.array(dim_middlemen[dim_middlemen['cluster_id'] == c][[\"latitude_middleman\",\"longitude_middleman\"]])\n",
    "        H_p = np.tile(H_p,(k,1))\n",
    "        capacities = dict(zip(H,dim_trucks[dim_trucks['cluster_id']==c]['capacity'].values))\n",
    "        mapping_trucks = dict(zip(H,trucks))\n",
    "        df_cluster_day = df_clusters[(df_clusters['pickup'] == d) & (df_clusters['cluster_id'] == c)]\n",
    "        n = len(df_cluster_day)\n",
    "        m = 1\n",
    "        if (n!=0):\n",
    "            N = ['n_'+str(i) for i in range(n)]\n",
    "            M = ['m_'+str(i) for i in range(m)]\n",
    "            quantities = {f: df_cluster_day[season+\"_volume\"].values[i] for i,f in enumerate(N)}\n",
    "            N_p = np.array(df_cluster_day[['latitude_farmer','longitude_farmer']])\n",
    "            vrp = VRPClass.VRP(H, N, H_p, N_p, quantities, capacities, 'road', M, M_p)\n",
    "            mapping = {h:(mapping_trucks[h],d, c) for h in H}\n",
    "            for i,ind in enumerate(df_cluster_day['plot_id']):\n",
    "                mapping[N[i]] = ind\n",
    "            n_trucks = dict(zip(H,[1]*len(H)))\n",
    "            file_dict = json_parser(H,N,H_p,N_p,capacities,quantities,mapping, vrp.distance, n_trucks)\n",
    "            with open( data_folder+\"instances/daily/daily_\"+\"cluster_\"+str(c)+\"_day_\"+str(d)+\".json\", \"wb\" ) as fp:\n",
    "                json.dump(file_dict, fp)\n",
    "            file_dict_pickle = {\n",
    "                \"H\":H,\n",
    "                \"N\":N,\n",
    "                \"M\":M,\n",
    "                \"H_p\":H_p,\n",
    "                \"N_p\":N_p,\n",
    "                \"M_p\":M_p,\n",
    "                \"capacities\":capacities,\n",
    "                \"quantities\":quantities,\n",
    "                \"mapping\":mapping,\n",
    "                \"distance\":vrp.distance,\n",
    "                \"type_dist\":\"road\",\n",
    "                \"n_trucks\":n_trucks,\n",
    "            }\n",
    "            pickle.dump( file_dict_pickle, open( data_folder+\"instances/daily/daily_\"+\"cluster_\"+str(c)+\"_day_\"+str(d)+\".p\", \"wb\" ) )\n",
    "            with open(bash_path, 'a') as file:\n",
    "                file.write(\"python2 /Users/sergiocamelo/Dropbox/Sergio-Joann/Code/PipelineScripts/load_and_solve_instance.py \"+data_folder+\"instances/daily/daily_\"+\"cluster_\"+str(c)+\"_day_\"+str(d) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create temporal instances\n",
    "bash_path = data_folder+\"instances/bash_temporal.sh\"\n",
    "open(bash_path, 'w').close()\n",
    "\n",
    "for c in clusters:\n",
    "    print(\"Processing config files of cluster: %d\" % (c),file=f)\n",
    "    M_p = np.array(dim_mills[dim_mills['code'] == 'SKIP'][[\"latitude_mill\",\"longitude_mill\"]].values)\n",
    "    trucks = dim_trucks[dim_trucks['cluster_id']==c]['truck_id'].values\n",
    "    trucks\n",
    "    k = len(trucks)\n",
    "    H = ['h_'+str(i) for i in range(k)]\n",
    "    H_p = np.array(dim_middlemen[dim_middlemen['cluster_id'] == c][[\"latitude_middleman\",\"longitude_middleman\"]])\n",
    "    H_p = np.tile(H_p,(k,1))\n",
    "    H_p\n",
    "    capacities = dict(zip(H,dim_trucks[dim_trucks['cluster_id']==c]['capacity'].values))\n",
    "    capacities\n",
    "    mapping_trucks = dict(zip(H,trucks))\n",
    "    df_cluster = df_clusters[(df_clusters['cluster_id'] == c)]\n",
    "\n",
    "    # Construct optimization problem\n",
    "    n = len(df_cluster)\n",
    "    m = 1\n",
    "\n",
    "    N = ['n_'+str(i) for i in range(n)]\n",
    "    M = ['m_'+str(i) for i in range(m)]\n",
    "\n",
    "\n",
    "    # Construct the mapping\n",
    "    mapping_trucks = dict(zip(H,trucks))\n",
    "    mapping = {h:(mapping_trucks[h],c) for h in H}\n",
    "    for i,ind in enumerate(df_cluster['plot_id']):\n",
    "        mapping[N[i]] = ind\n",
    "\n",
    "    quantities = {f: df_cluster[season_volume].values[i] for i,f in enumerate(N)}\n",
    "    N_p = np.array(df_cluster[['latitude_farmer','longitude_farmer']])\n",
    "    vrp = VRPClass.VRP(H, N, H_p, N_p, quantities, capacities, 'road', M, M_p)\n",
    "    n_trucks = dict(zip(H,[days_analysis]*len(H)))\n",
    "    file_dict = json_parser(H,N,H_p,N_p,capacities,quantities,mapping, vrp.distance, n_trucks)\n",
    "    with open( data_folder+\"instances/temporal/temporal_\"+\"cluster_\"+str(c)+\".json\", \"wb\" ) as fp:\n",
    "        json.dump(file_dict, fp)\n",
    "    file_dict_pickle = {\n",
    "        \"H\":H,\n",
    "        \"N\":N,\n",
    "        \"M\":M,\n",
    "        \"H_p\":H_p,\n",
    "        \"N_p\":N_p,\n",
    "        \"M_p\":M_p,\n",
    "        \"capacities\":capacities,\n",
    "        \"quantities\":quantities,\n",
    "        \"mapping\":mapping,\n",
    "        \"distance\":vrp.distance,\n",
    "        \"type_dist\":\"road\",\n",
    "        \"n_trucks\":n_trucks\n",
    "    }\n",
    "    pickle.dump( file_dict_pickle, open( data_folder+\"instances/temporal/temporal_\"+\"cluster_\"+str(c)+\".p\", \"wb\" ) )\n",
    "    with open(bash_path, 'a') as file:\n",
    "        file.write(\"python2 /Users/sergiocamelo/Dropbox/Sergio-Joann/Code/PipelineScripts/load_and_solve_instance.py \"+data_folder+\"instances/temporal/temporal_\"+\"cluster_\"+str(c) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "26\n",
      "9\n",
      "30\n",
      "18\n",
      "17\n",
      "20\n",
      "35\n",
      "26\n",
      "25\n",
      "25\n",
      "28\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# We now solve the spatial problem\n",
    "# Results spatial\n",
    "bash_path = data_folder+\"instances/bash_spatial.sh\"\n",
    "open(bash_path, 'w').close()\n",
    "\n",
    "# Extract all trucks that are available\n",
    "capacities = {}\n",
    "H_p = np.empty([0,2])\n",
    "H = []; i = 0\n",
    "truck_mapping = {}\n",
    "for c in clusters:\n",
    "    middle_df_c = dim_middlemen[dim_middlemen.cluster_id == c]\n",
    "    trucks = dim_trucks[dim_trucks['cluster_id']==c]['truck_id'].values\n",
    "    capacities_v = dim_trucks[dim_trucks['cluster_id']==c]['capacity'].values\n",
    "    for j,t in enumerate(trucks):\n",
    "        H_p = np.vstack([H_p, np.array(middle_df_c[['latitude_middleman','longitude_middleman']])])\n",
    "        H.append('h_'+str(i))\n",
    "        truck_mapping['h_'+str(i)] = (t,c)\n",
    "        capacities['h_'+str(i)] = capacities_v[j]\n",
    "        i += 1\n",
    "k = len(capacities)\n",
    "M_p = np.array(dim_mills[dim_mills['code'] == 'SKIP'][[\"latitude_mill\",\"longitude_mill\"]].values)\n",
    "for d in range(days_analysis):\n",
    "    print(\"Processing config files of dat: %d\" % (d),file=f)\n",
    "    df_day = df_clusters[(df_clusters['pickup'] == d)]\n",
    "    print(len(df_day))\n",
    "    n = len(df_day)\n",
    "    m = 1\n",
    "    N = ['n_'+str(i) for i in range(n)]\n",
    "    M = ['m_'+str(i) for i in range(m)]\n",
    "    mapping = {}\n",
    "    for h in H:\n",
    "        mapping[h] = (truck_mapping[h][0],d,truck_mapping[h][1])\n",
    "    for i,ind in enumerate(df_day['plot_id']):\n",
    "        mapping[N[i]] = ind\n",
    "    # Construct optimization problem\n",
    "    quantities = {f: df_day[season_volume].values[i] for i,f in enumerate(N)}\n",
    "    N_p = np.array(df_day[['latitude_farmer','longitude_farmer']])\n",
    "    vrp = VRPClass.VRP(H, N, H_p, N_p, quantities, capacities, 'road', M, M_p)\n",
    "    n_trucks = dict(zip(H,[1]*len(H)))\n",
    "\n",
    "    file_dict = json_parser(H,N,H_p,N_p,capacities,quantities,mapping, vrp.distance,n_trucks)\n",
    "\n",
    "\n",
    "    with open( data_folder+\"instances/spatial/spatial_\"+\"day_\"+str(d)+\".json\", \"wb\" ) as fp:\n",
    "        json.dump(file_dict, fp)\n",
    "\n",
    "    file_dict_pickle = {\n",
    "        \"H\":H,\n",
    "        \"N\":N,\n",
    "        \"M\":M,\n",
    "        \"H_p\":H_p,\n",
    "        \"N_p\":N_p,\n",
    "        \"M_p\":M_p,\n",
    "        \"capacities\":capacities,\n",
    "        \"quantities\":quantities,\n",
    "        \"mapping\":mapping,\n",
    "        \"distance\":vrp.distance,\n",
    "        \"type_dist\":\"road\",\n",
    "        \"n_trucks\":n_trucks\n",
    "    }\n",
    "\n",
    "    pickle.dump( file_dict_pickle, open( data_folder+\"instances/spatial/spatial_\"+\"day_\"+str(d)+\".p\", \"wb\" ) )\n",
    "\n",
    "    with open(bash_path, 'a') as file:\n",
    "        file.write(\"python2 /Users/sergiocamelo/Dropbox/Sergio-Joann/Code/PipelineScripts/load_and_solve_instance.py \"+data_folder+\"instances/spatial/spatial_\"+\"day_\"+str(d) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create bash scripts for the cluster\n",
    "\n",
    "header = \"\"\"#!/bin/bash\n",
    "#\n",
    "#SBATCH --job-name=test\n",
    "#\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem-per-cpu=2G\n",
    "\n",
    "cd ~/vrp_project\n",
    "source ~/.virtualenvs/vrp_project/bin/activate\n",
    "module load gurobi/8.0.1_py27\n",
    "srun python ~/vrp_project/Code/PipelineScripts/load_and_solve_instance.py ~/vrp_project/StandardizedData/instances/%s/%s -p normal,dev,gpu\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "folder_path = data_folder + 'instances/cluster/'\n",
    "cluster_path = \"~/vrp_project/StandardizedData/instances/cluster/\"\n",
    "\n",
    "\n",
    "submission = folder_path + 'submission_daily.sh'\n",
    "open(submission, 'w').close()\n",
    "\n",
    "for c in clusters:\n",
    "    for d in range(days_analysis):\n",
    "        instance = \"daily_\"+\"cluster_\"+str(c)+\"_day_\"+str(d)\n",
    "        path = folder_path + instance + '.sh'\n",
    "        open(path, 'w').close()\n",
    "        with open(path, 'a') as file:\n",
    "            file.write(header % (\"daily\",instance))\n",
    "        with open(submission, 'a') as file:\n",
    "            file.write('sbatch ' + cluster_path + instance + '.sh' + '\\n')\n",
    "\n",
    "submission = folder_path + 'submission_temporal.sh'\n",
    "open(submission, 'w').close()\n",
    "for c in clusters:\n",
    "    instance = \"temporal_\"+\"cluster_\"+str(c)\n",
    "    path = folder_path + instance + '.sh'\n",
    "    open(path, 'w').close()\n",
    "    with open(path, 'a') as file:\n",
    "        file.write(header % (\"temporal\",instance))\n",
    "    with open(submission, 'a') as file:\n",
    "        file.write('sbatch ' + cluster_path + instance + '.sh' + '\\n')\n",
    "        \n",
    "submission = folder_path + 'submission_spatial.sh'\n",
    "open(submission, 'w').close()\n",
    "for d in range(days_analysis):\n",
    "    instance = \"spatial_\"+\"day_\"+str(d)\n",
    "    path = folder_path + instance + '.sh'\n",
    "    open(path, 'w').close()\n",
    "    with open(path, 'a') as file:\n",
    "        file.write(header % (\"spatial\",instance))\n",
    "    with open(submission, 'a') as file:\n",
    "        file.write('sbatch ' + cluster_path + instance + '.sh' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"#!/bin/bash\\n#\\n#SBATCH --job-name=test\\n#\\n#SBATCH --time=10:00\\n#SBATCH --ntasks=1\\n#SBATCH --cpus-per-task=1\\n#SBATCH --mem-per-cpu=2G\\n\\nsource ~/.virtualenvs/vrp_project/bin/activate\\nmodule load gurobi/8.0.1_py27\\nsrun python ~/vrp_project/Code/PipelineScripts/load_and_solve_instance.py ~/vrp_project/StandardizedData/instances/%s/%s\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
