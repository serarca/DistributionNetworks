{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "import math\n",
    "from dateutil.parser import parse\n",
    "from geopandas import GeoDataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import json\n",
    "import ast\n",
    "import sys\n",
    "import pickle\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "\n",
    "folder_project = \"/Users/sergiocamelo/Dropbox/Sergio-Joann/\"\n",
    "os.chdir(folder_project)\n",
    "\n",
    "sys.path.insert(0, 'Code/VRPEngine/pyCode')\n",
    "sys.path.insert(0, 'Code/VRPEngine/C++Engine')\n",
    "sys.path.insert(0, 'Code/VRPEngine/pyCode/tsp')\n",
    "\n",
    "import solver as solver\n",
    "import distances as distances\n",
    "import VRPClass\n",
    "import road_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days_analysis = 14\n",
    "ts = 'Jan142018'\n",
    "data_folder = 'StandardizedData/'\n",
    "results_folder_ts = 'Results/'+ ts + \"/\" \n",
    "folder_instances = results_folder_ts+\"instances\"\n",
    "solution_folder = results_folder_ts+'solution_routes/'\n",
    "dim_farmers_file = 'dim_farmers_sample.csv'\n",
    "dim_all_farmers_file = 'dim_all_farmers.csv'\n",
    "dim_confirmed_farmers_file = 'dim_farmers_confirmed_old.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truck_loads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get coordinates of farmers, mills and trucks\n",
    "dim_pickups = pd.read_csv(results_folder_ts+'data_cleaning_results/dim_pickups.csv')\n",
    "\n",
    "dim_trucks = pd.read_csv(results_folder_ts+'data_cleaning_results/dim_trucks.csv')\n",
    "dim_farmers = pd.read_csv(data_folder+dim_farmers_file)\n",
    "dim_all_farmers = pd.read_csv(data_folder+dim_all_farmers_file)\n",
    "dim_confirmed_farmers = pd.read_csv(data_folder+dim_confirmed_farmers_file)\n",
    "\n",
    "\n",
    "dim_mills = pd.read_csv(data_folder+'dim_mills.csv')\n",
    "dim_middlemen = pd.read_csv(data_folder+'dim_middlemen.csv')\n",
    "\n",
    "positions_dict = {}\n",
    "for i, row in dim_mills.iterrows():\n",
    "    positions_dict[row[\"code\"]] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_all_farmers.iterrows():\n",
    "    positions_dict[row[\"farmer_id\"]+'-'+str(row[\"plot_number\"])] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_confirmed_farmers.iterrows():\n",
    "    positions_dict[row[\"farmer_id\"]+'-'+str(row[\"plot_number\"])] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_farmers.iterrows():\n",
    "    positions_dict[row[\"farmer_id\"]+'-'+str(row[\"plot_number\"])] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_middlemen.iterrows():\n",
    "    positions_dict[row[\"cluster_id\"]] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_trucks.iterrows():\n",
    "    positions_dict[row[\"truck_id\"]] = positions_dict[float(row[\"cluster_id\"])]\n",
    "    \n",
    "def get_distance(a, b, mapping = None, detailed = True):\n",
    "    if a[0] == 'h':\n",
    "        m_a = mapping[a][0]\n",
    "    elif a[0] == 'n':\n",
    "        m_a = mapping[a]\n",
    "    else:\n",
    "        m_a = a\n",
    "    if b[0] == 'h':\n",
    "        m_b = mapping[b][0]\n",
    "    elif b[0] == 'n':\n",
    "        m_b = mapping[b]\n",
    "    else:\n",
    "        m_b = b\n",
    "    return road_distance.road_distance(positions_dict[m_a],positions_dict[m_b], detailed=detailed)\n",
    "\n",
    "def path_distance(path, mapping, mill):\n",
    "    surfaces = ['access','gravel','asphalt','dirt','unknown']\n",
    "    new_path = copy.copy(path)\n",
    "    new_path.insert(len(path)-1, mill)\n",
    "    result = None\n",
    "    for i in range(len(new_path)-1):\n",
    "        if not result:\n",
    "            result = get_distance(new_path[i],new_path[i+1],mapping)\n",
    "            #ipdb.set_trace()\n",
    "        else:\n",
    "            dist = get_distance(new_path[i],new_path[i+1],mapping)\n",
    "\n",
    "            result[\"distance\"] += dist[\"distance\"]\n",
    "            result[\"time\"] += dist[\"time\"]\n",
    "            for k in dist[\"detailed_distance\"].keys():\n",
    "                if k in result[\"detailed_distance\"].keys():\n",
    "                    result[\"detailed_distance\"][k]+=dist[\"detailed_distance\"][k]\n",
    "                    result[\"detailed_time\"][k]+=dist[\"detailed_time\"][k]\n",
    "                else:\n",
    "                    result[\"detailed_distance\"][k]=dist[\"detailed_distance\"][k]\n",
    "                    result[\"detailed_time\"][k]=dist[\"detailed_time\"][k]\n",
    "    \n",
    "    for s in surfaces:\n",
    "        if not(s in result[\"detailed_distance\"].keys()):\n",
    "            result[\"detailed_distance\"][s] = 0\n",
    "            result[\"detailed_time\"][s] = 0\n",
    "    \n",
    "    return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dict(d):\n",
    "    d = ast.literal_eval(d)\n",
    "    for k in d.keys():\n",
    "        if d[k] == 0:\n",
    "            del d[k]\n",
    "    return d\n",
    "dim_middlemen = dim_middlemen[pd.notnull(dim_middlemen['cluster_id'])]\n",
    "dim_middlemen['cluster_id'] = dim_middlemen['cluster_id'].apply(int).apply(str)\n",
    "dim_middlemen['trucks'] = dim_middlemen['trucks'].apply(clean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove repeated farmers\n",
    "def remove_repetitions(data,N,vrp):\n",
    "    \n",
    "    # Recalculate distances and verify that they are the same\n",
    "    new_ub = 0\n",
    "    for t in data['routes'].keys():\n",
    "        for i,route in enumerate(data['routes'][t]):\n",
    "            route[\"distance\"] = vrp.distance_path([t]+route['route']+[t])\n",
    "            new_ub += route[\"distance\"]\n",
    "    if (np.abs(new_ub-data['z_ub']) > 0.1):\n",
    "        print(\"Error in distance calculation\", new_ub,data['z_ub'])\n",
    "    \n",
    "    # Check which farmers are picked up\n",
    "    ocurrences = dict(zip(N,[0]*len(N)))\n",
    "    for t in data['routes'].keys():\n",
    "        for route in data['routes'][t]:\n",
    "            for n in route['route']:\n",
    "                ocurrences[n]+=1\n",
    "\n",
    "    for n in N:\n",
    "        if ocurrences[n]>1:\n",
    "            \n",
    "            print(\"Node %s is repeated\" % n)\n",
    "            # Calculate gain from removing it\n",
    "            min_gain = float('inf')\n",
    "            min_route = None\n",
    "            min_coord = None\n",
    "            for t in data['routes'].keys():\n",
    "                for i,route in enumerate(data['routes'][t]):\n",
    "                    if n in route['route']:\n",
    "                        route_with = route['route']\n",
    "                        route_without = [m for m in route['route'] if m!=n]\n",
    "                        dist_before = vrp.distance_path([t]+route_with+[t])\n",
    "                        dist_after = vrp.distance_path([t]+route_without+[t])\n",
    "                        gain = dist_before - dist_after\n",
    "                        if gain <= min_gain:\n",
    "                            min_gain = gain\n",
    "                            min_route = copy.copy(route_with)\n",
    "                            min_coord = (t,i)\n",
    "                        route['route'] = route_without\n",
    "            data['routes'][min_coord[0]][min_coord[1]]['route'] = min_route\n",
    "            \n",
    "            for t in data['routes'].keys():\n",
    "                for i,route in enumerate(data['routes'][t]):\n",
    "                    route[\"distance\"] = vrp.distance_path([t]+route['route']+[t])\n",
    "                    new_ub += route[\"distance\"]\n",
    "            data['z_ub'] = new_ub  \n",
    "        if ocurrences[n]<1:\n",
    "            print(\"Error, there is a node without pickup route\")\n",
    "\n",
    "\n",
    "    ocurrences = dict(zip(N,[0]*len(N)))\n",
    "    for t in data['routes'].keys():\n",
    "        for route in data['routes'][t]:\n",
    "            for n in route['route']:\n",
    "                ocurrences[n]+=1\n",
    "\n",
    "    for n in N:\n",
    "        if ocurrences[n]>1:\n",
    "            print(\"Error, fixing not successful\")\n",
    "        if ocurrences[n]<1:\n",
    "            print(\"Error, fixing not successful\")\n",
    "            \n",
    "            \n",
    "    # Recalculate Upper Bound\n",
    "    old_ub = data['z_ub']\n",
    "    new_ub = 0\n",
    "    for t in data['routes'].keys():\n",
    "        for i,route in enumerate(data['routes'][t]):\n",
    "            route[\"distance\"] = vrp.distance_path([t]+route['route']+[t])\n",
    "            new_ub += route[\"distance\"]\n",
    "    data['z_ub'] = new_ub  \n",
    "    if old_ub < new_ub - 0.1:\n",
    "        print(\"Error in new upper bound\")\n",
    "        print(old_ub, new_ub)\n",
    "    try:\n",
    "        gap = (data['z_ub']-data['z_lb'])/data['z_ub']\n",
    "        if gap < -10**(-8):\n",
    "            print(\"Error, negative gap\")\n",
    "            print(data['z_ub'])\n",
    "            print(data['z_lb'])\n",
    "    except:\n",
    "        print(\"zero z_ub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of farmer to original middleman, to check whether they are retained or not\n",
    "original_middleman = dict(zip(dim_farmers['farmer_id']+'-'+dim_farmers['plot_number'].apply(str),dim_farmers['cluster_id']))\n",
    "for i,row in dim_pickups.iterrows():\n",
    "    original_middleman[row['plot_id']] = row['cluster_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 278 daily instances\n",
      "14332613.574851997\n",
      "13992249.0753\n",
      "0.02374755293592939\n"
     ]
    }
   ],
   "source": [
    "# Load daily results\n",
    "instances = glob.glob(solution_folder+\"/daily/*.json\")\n",
    "print(\"There is a total of %d daily instances\"%len(instances))\n",
    "daily_results = {}\n",
    "instances_parsed = []\n",
    "\n",
    "z_ub = 0\n",
    "z_lb = 0\n",
    "for instance in instances:\n",
    "    # Load instance\n",
    "    with open(instance) as f:\n",
    "        data = json.load(f)\n",
    "    name_instance = re.search('/daily/(.*).json',instance).group(1)\n",
    "\n",
    "    # Get day and cluster\n",
    "    day = name_instance.split('_')[4]\n",
    "    cluster = name_instance.split('_')[2]\n",
    "    \n",
    "    # Get the data of this instance\n",
    "    instance_data = pickle.load( open( folder_instances+\"/daily/daily_cluster_\"+str(cluster)+\"_day_\"+str(day) + \".p\", \"rb\" ) )\n",
    "    locals().update(instance_data)\n",
    "    vrp = VRPClass.VRP(H, N, H_p, N_p, quantities, capacities, type_dist, M = M, M_p = M_p, distance_matrix = distance)\n",
    "    data['mapping'] = mapping\n",
    "\n",
    "    # Get farmers served\n",
    "    farmers = [node for node in data['mapping'].keys() if node[0] == 'n']\n",
    "    farmers_index = [data['mapping'][f] for f in farmers]\n",
    "    # Make sure these farmers are part of the routes\n",
    "    farmers_inroute = [n for node in [route['route'] for key, routes in data['routes'].iteritems() if len(routes)>0 for route in routes] for n in node]\n",
    "    if (set(farmers_inroute)!=set(farmers)):\n",
    "        print(\"error\")\n",
    "        \n",
    "    # Fix farmers that are picked up more than once\n",
    "    remove_repetitions(data,N,vrp)\n",
    "    \n",
    "    instances_parsed.append({\n",
    "        \"n_nodes\": len([n for n in data['mapping'].keys() if n[0]=='n']),\n",
    "        \"cluster\": cluster,\n",
    "        \"day\": day,\n",
    "        \"n_trucks\": len([n for n in data['mapping'].keys() if n[0]=='h']),\n",
    "        \"z_ub\":data['z_ub'],\n",
    "        \"z_lb\":data['z_lb'],\n",
    "        \"gap\":(data['z_ub']-data['z_lb'])/data['z_ub'],\n",
    "        \"nodes_served\":[data['mapping'][n] for n in data['mapping'].keys() if n[0]=='n']\n",
    "    })\n",
    "    \n",
    "    for t in data['routes'].keys():\n",
    "        for route in data['routes'][t]:\n",
    "            detailed_dist = path_distance([t]+route['route']+[t],mapping,'SKIP')\n",
    "            # The route with the real ids\n",
    "            real_route = [data['mapping'][t][0]]+[data['mapping'][n] for n in route[\"route\"] if n[0]=='n']+['SKIP']+[data['mapping'][t][0]]\n",
    "            truck_loads.append({\n",
    "                    \"type\":\"daily\",\n",
    "                    \"cluster\":data['mapping'][t][2],\n",
    "                    \"truck\":data['mapping'][t][0],\n",
    "                    \"day\":data['mapping'][t][1],\n",
    "                    \"max_load\":route[\"max_load\"],\n",
    "                    \"load\":np.sum([quantities[n] for n in route['route']]),\n",
    "                    \"load_perc\":np.sum([quantities[n] for n in route['route']])/(route[\"max_load\"]+0.0),\n",
    "                    \"n_farmers\":len(set(route[\"route\"])),\n",
    "                    \"distance\":vrp.distance_path([t]+route['route']+[t]),\n",
    "                    \"detailed_distance\":detailed_dist['distance'],\n",
    "                    \"gravel\":detailed_dist['detailed_distance']['gravel'],\n",
    "                    \"asphalt\":detailed_dist['detailed_distance']['asphalt'],\n",
    "                    \"access\":detailed_dist['detailed_distance']['access'],\n",
    "                    \"dirt\":detailed_dist['detailed_distance']['dirt'],\n",
    "                    \"unknown\":detailed_dist['detailed_distance']['unknown'],\n",
    "                    \"road_time\":detailed_dist['time'],\n",
    "                    \"farmers\":[data['mapping'][n] for n in route[\"route\"] if n[0]=='n'],\n",
    "                    \"route\":real_route,\n",
    "                    # This contains the node and its location\n",
    "                    \"waypoints\":[(r,positions_dict[r]) for r in real_route],\n",
    "                    \"retained_farmers\":np.sum(np.array([original_middleman[data['mapping'][n]] for n in route[\"route\"] if n[0]=='n'] ) == data['mapping'][t][2])\n",
    "                })\n",
    "    #print(instance)\n",
    "    #print((data['z_ub']-data['z_lb'])/data['z_ub'])\n",
    "\n",
    "    \n",
    "    # Get upper and lower bounds\n",
    "    z_ub += data['z_ub']\n",
    "    z_lb += data['z_lb']\n",
    "print (z_ub)\n",
    "print (z_lb)\n",
    "print((z_ub-z_lb)/z_ub)\n",
    "daily_df = pd.DataFrame(instances_parsed)\n",
    "daily_df.to_csv(results_folder_ts+(\"tables_for_report/runs_daily.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 14 spatial instances\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_0.json\n",
      "Node n_4 is repeated\n",
      "Node n_39 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_1.json\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_10.json\n",
      "Node n_50 is repeated\n",
      "Node n_54 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_11.json\n",
      "Node n_34 is repeated\n",
      "Node n_38 is repeated\n",
      "Node n_43 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_12.json\n",
      "Node n_1 is repeated\n",
      "Node n_7 is repeated\n",
      "Node n_52 is repeated\n",
      "Node n_55 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_13.json\n",
      "Node n_1 is repeated\n",
      "Node n_66 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_2.json\n",
      "Node n_0 is repeated\n",
      "Node n_1 is repeated\n",
      "Node n_30 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_3.json\n",
      "Node n_1 is repeated\n",
      "Node n_26 is repeated\n",
      "Node n_35 is repeated\n",
      "Node n_37 is repeated\n",
      "Node n_58 is repeated\n",
      "Node n_63 is repeated\n",
      "Node n_74 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_4.json\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_5.json\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_6.json\n",
      "Node n_19 is repeated\n",
      "Node n_36 is repeated\n",
      "Node n_61 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_7.json\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_8.json\n",
      "Node n_0 is repeated\n",
      "Node n_5 is repeated\n",
      "Node n_27 is repeated\n",
      "Node n_55 is repeated\n",
      "Node n_63 is repeated\n",
      "Results/Jan142018/solution_routes/spatial/spatial_day_9.json\n",
      "Node n_29 is repeated\n",
      "8956604.56177\n",
      "6976239.35127\n",
      "0.22110669247932752\n"
     ]
    }
   ],
   "source": [
    "# Load spatial results\n",
    "instances = glob.glob(solution_folder+\"spatial/*.json\")\n",
    "print(\"There is a total of %d spatial instances\"%len(instances))\n",
    "daily_results = {}\n",
    "instances_parsed = []\n",
    "\n",
    "\n",
    "z_ub = 0\n",
    "z_lb = 0\n",
    "\n",
    "for instance in instances:\n",
    "    print(instance)\n",
    "    # Load instance\n",
    "    with open(instance) as fi:\n",
    "        data = json.load(fi)\n",
    "    name_instance = re.search('/spatial/(.*).json',instance).group(1)\n",
    "\n",
    "    # Get day and cluster\n",
    "    day = name_instance.split('_')[2]\n",
    "    \n",
    "    # Get the data of this instance\n",
    "    instance_data = pickle.load( open( folder_instances+\"/spatial/spatial_day_\"+str(day) + \".p\", \"rb\" ) )\n",
    "    locals().update(instance_data)\n",
    "    vrp = VRPClass.VRP(H, N, H_p, N_p, quantities, capacities, type_dist, M = M, M_p = M_p, distance_matrix = distance)\n",
    "\n",
    "\n",
    "    # Get farmers served\n",
    "    farmers = [node for node in data['mapping'].keys() if node[0] == 'n']\n",
    "    farmers_index = [data['mapping'][f] for f in farmers]\n",
    "    # Make sure these farmers are part of the routes\n",
    "    farmers_inroute = [n for node in [route['route'] for key, routes in data['routes'].iteritems() if len(routes)>0 for route in routes] for n in node]\n",
    "    if (set(farmers_inroute)!=set(farmers)):\n",
    "        print(\"error\")\n",
    "        print(farmers_inroute)\n",
    "        print(farmers)\n",
    "        print(name_instance)\n",
    "        print (data['z_lb'])\n",
    "        print (data['z_ub'])\n",
    "        \n",
    "    # Fix farmers that are picked up more than once\n",
    "    remove_repetitions(data,N,vrp)\n",
    "    \n",
    "    # Get upper and lower bounds\n",
    "    z_ub += data['z_ub']\n",
    "    z_lb += data['z_lb']\n",
    "    \n",
    "    try:\n",
    "        instances_parsed.append({\n",
    "            \"n_nodes\": len([n for n in data['mapping'].keys() if n[0]=='n']),\n",
    "            \"day\": day,\n",
    "            \"n_trucks\": len([n for n in data['mapping'].keys() if n[0]=='h']),\n",
    "            \"z_ub\":data['z_ub'],\n",
    "            \"z_lb\":data['z_lb'],\n",
    "            \"gap\":(data['z_ub']-data['z_lb'])/data['z_ub'],\n",
    "            \"nodes_served\":[data['mapping'][n] for n in data['mapping'].keys() if n[0]=='n']\n",
    "        })\n",
    "    except:\n",
    "        print(\"zero z_ub\")\n",
    "    \n",
    "    for t in data['routes'].keys():\n",
    "        for route in data['routes'][t]:\n",
    "            detailed_dist = path_distance([t]+route['route']+[t],mapping,'SKIP')\n",
    "            real_route = [data['mapping'][t][0]]+[data['mapping'][n] for n in route[\"route\"] if n[0]=='n']+['SKIP']+[data['mapping'][t][0]]\n",
    "            truck_loads.append({\n",
    "                    \"type\":\"spatial\",\n",
    "                    \"cluster\":data['mapping'][t][2],\n",
    "                    \"truck\":data['mapping'][t][0],\n",
    "                    \"day\":data['mapping'][t][1],\n",
    "                    \"max_load\":route[\"max_load\"],\n",
    "                    \"load\":np.sum([quantities[n] for n in route['route']]),\n",
    "                    \"load_perc\":np.sum([quantities[n] for n in route['route']])/(route[\"max_load\"]+0.0),\n",
    "                    \"n_farmers\":len(set(route[\"route\"])),\n",
    "                    \"distance\":vrp.distance_path([t]+route['route']+[t]),\n",
    "                    \"detailed_distance\":detailed_dist['distance'],\n",
    "                    \"gravel\":detailed_dist['detailed_distance']['gravel'],\n",
    "                    \"asphalt\":detailed_dist['detailed_distance']['asphalt'],\n",
    "                    \"access\":detailed_dist['detailed_distance']['access'],\n",
    "                    \"dirt\":detailed_dist['detailed_distance']['dirt'],\n",
    "                    \"unknown\":detailed_dist['detailed_distance']['unknown'],\n",
    "                    \"road_time\":detailed_dist['time'],\n",
    "                    \"farmers\":[data['mapping'][n] for n in route[\"route\"] if n[0]=='n'],\n",
    "                    \"route\":real_route,\n",
    "                    # This contains the node and its location\n",
    "                    \"waypoints\":[(r,positions_dict[r]) for r in real_route],\n",
    "                    \"retained_farmers\":np.sum(np.array([original_middleman[data['mapping'][n]] for n in route[\"route\"] if n[0]=='n'] ) == data['mapping'][t][2])\n",
    "            })\n",
    "print (z_ub)\n",
    "print (z_lb)\n",
    "print((z_ub-z_lb)/z_ub)\n",
    "spatial_df = pd.DataFrame(instances_parsed)\n",
    "\n",
    "spatial_df.to_csv(results_folder_ts+(\"tables_for_report/runs_spatial.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add missing routes\n",
    "days_analysis = 14\n",
    "data_folder = 'StandardizedData/'\n",
    "results_folder_ts = 'Results/'+ '2018-10-20_17:58' + \"/\" \n",
    "folder_instances = results_folder_ts+\"instances\"\n",
    "solution_folder = results_folder_ts+'solution_routes/'\n",
    "\n",
    "# Get coordinates of farmers, mills and trucks\n",
    "dim_trucks = pd.read_csv(data_folder+'dim_trucks.csv')\n",
    "dim_farmers = pd.read_csv(data_folder+'dim_farmers.csv')\n",
    "dim_mills = pd.read_csv(data_folder+'dim_mills.csv')\n",
    "dim_middlemen = pd.read_csv(data_folder+'dim_middlemen.csv')\n",
    "\n",
    "positions_dict = {}\n",
    "for i, row in dim_mills.iterrows():\n",
    "    positions_dict[row[\"code\"]] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_farmers.iterrows():\n",
    "    positions_dict[row[\"farmer_id\"]+'-'+str(row[\"plot_number\"])] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_middlemen.iterrows():\n",
    "    positions_dict[row[\"cluster_id\"]] = (row['latitude'], row['longitude'])\n",
    "for i, row in dim_trucks.iterrows():\n",
    "    positions_dict[row[\"truck_id\"]] = positions_dict[float(row[\"cluster_id\"])]\n",
    "    \n",
    "# Load spatial results\n",
    "instances = ['Results/2018-10-20_17:58/solution_routes/spatial/spatial_day_0.json','Results/2018-10-20_17:58/solution_routes/spatial/spatial_day_1.json']\n",
    "\n",
    "\n",
    "\n",
    "z_ub = 0\n",
    "z_lb = 0\n",
    "\n",
    "for instance in instances:\n",
    "    print(instance)\n",
    "    # Load instance\n",
    "    with open(instance) as fi:\n",
    "        data = json.load(fi)\n",
    "    name_instance = re.search('/spatial/(.*).json',instance).group(1)\n",
    "\n",
    "    # Get day and cluster\n",
    "    day = name_instance.split('_')[2]\n",
    "    \n",
    "    # Get the data of this instance\n",
    "    instance_data = pickle.load( open( folder_instances+\"/spatial/spatial_day_\"+str(day) + \".p\", \"rb\" ) )\n",
    "    locals().update(instance_data)\n",
    "    vrp = VRPClass.VRP(H, N, H_p, N_p, quantities, capacities, type_dist, M = M, M_p = M_p, distance_matrix = distance)\n",
    "\n",
    "\n",
    "    # Get farmers served\n",
    "    farmers = [node for node in data['mapping'].keys() if node[0] == 'n']\n",
    "    farmers_index = [data['mapping'][f] for f in farmers]\n",
    "    # Make sure these farmers are part of the routes\n",
    "    farmers_inroute = [n for node in [route['route'] for key, routes in data['routes'].iteritems() if len(routes)>0 for route in routes] for n in node]\n",
    "    if (set(farmers_inroute)!=set(farmers)):\n",
    "        print(\"error\")\n",
    "        print(farmers_inroute)\n",
    "        print(farmers)\n",
    "        print(name_instance)\n",
    "        print (data['z_lb'])\n",
    "        print (data['z_ub'])\n",
    "        \n",
    "    # Fix farmers that are picked up more than once\n",
    "    remove_repetitions(data,N,vrp)\n",
    "    \n",
    "    # Get upper and lower bounds\n",
    "    z_ub += data['z_ub']\n",
    "    z_lb += data['z_lb']\n",
    "    \n",
    "    try:\n",
    "        instances_parsed.append({\n",
    "            \"n_nodes\": len([n for n in data['mapping'].keys() if n[0]=='n']),\n",
    "            \"day\": day,\n",
    "            \"n_trucks\": len([n for n in data['mapping'].keys() if n[0]=='h']),\n",
    "            \"z_ub\":data['z_ub'],\n",
    "            \"z_lb\":data['z_lb'],\n",
    "            \"gap\":(data['z_ub']-data['z_lb'])/data['z_ub'],\n",
    "            \"nodes_served\":[data['mapping'][n] for n in data['mapping'].keys() if n[0]=='n']\n",
    "        })\n",
    "    except:\n",
    "        print(\"zero z_ub\")\n",
    "    \n",
    "    for t in data['routes'].keys():\n",
    "        for route in data['routes'][t]:\n",
    "            detailed_dist = path_distance([t]+route['route']+[t],mapping,'SKIP')\n",
    "            real_route = [data['mapping'][t][0]]+[data['mapping'][n] for n in route[\"route\"] if n[0]=='n']+['SKIP']+[data['mapping'][t][0]]\n",
    "            truck_loads.append({\n",
    "                    \"type\":\"spatial\",\n",
    "                    \"cluster\":data['mapping'][t][2],\n",
    "                    \"truck\":data['mapping'][t][0],\n",
    "                    \"day\":data['mapping'][t][1],\n",
    "                    \"max_load\":route[\"max_load\"],\n",
    "                    \"load\":np.sum([quantities[n] for n in route['route']]),\n",
    "                    \"load_perc\":np.sum([quantities[n] for n in route['route']])/(route[\"max_load\"]+0.0),\n",
    "                    \"n_farmers\":len(set(route[\"route\"])),\n",
    "                    \"distance\":vrp.distance_path([t]+route['route']+[t]),\n",
    "                    \"detailed_distance\":detailed_dist['distance'],\n",
    "                    \"gravel\":detailed_dist['detailed_distance']['gravel'],\n",
    "                    \"asphalt\":detailed_dist['detailed_distance']['asphalt'],\n",
    "                    \"access\":detailed_dist['detailed_distance']['access'],\n",
    "                    \"dirt\":detailed_dist['detailed_distance']['dirt'],\n",
    "                    \"unknown\":detailed_dist['detailed_distance']['unknown'],\n",
    "                    \"road_time\":detailed_dist['time'],\n",
    "                    \"farmers\":[data['mapping'][n] for n in route[\"route\"] if n[0]=='n'],\n",
    "                    \"route\":real_route,\n",
    "                    # This contains the node and its location\n",
    "                    \"waypoints\":[(r,positions_dict[r]) for r in real_route],\n",
    "                    \"retained_farmers\":np.sum(np.array([original_middleman[data['mapping'][n]] for n in route[\"route\"] if n[0]=='n'] ) == data['mapping'][t][2])\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Load temporal results\n",
    "# instances = glob.glob(solution_folder+\"temporal/*.json\")\n",
    "# print(\"There is a total of %d daily instances\"%len(instances))\n",
    "# daily_results = {}\n",
    "# instances_parsed = []\n",
    "\n",
    "\n",
    "# z_ub = 0\n",
    "# z_lb = 0\n",
    "# for instance in instances:\n",
    "#     print(instance)\n",
    "#     # Load instance\n",
    "#     with open(instance) as f:\n",
    "#         data = json.load(f)\n",
    "#     name_instance = re.search('/temporal/(.*).json',instance).group(1)\n",
    "\n",
    "#     # Get day and cluster\n",
    "#     cluster = name_instance.split('_')[2]\n",
    "    \n",
    "#     # Get the data of this instance\n",
    "#     instance_data = pickle.load( open( folder_instances+\"/temporal/temporal_cluster_\"+str(cluster) + \".p\", \"rb\" ) )\n",
    "#     locals().update(instance_data)\n",
    "#     vrp = VRPClass.VRP(H, N, H_p, N_p, quantities, capacities, type_dist, M = M, M_p = M_p, distance_matrix = distance)\n",
    "\n",
    "\n",
    "#     # Get farmers served\n",
    "#     farmers = [node for node in data['mapping'].keys() if node[0] == 'n']\n",
    "#     farmers_index = [data['mapping'][f] for f in farmers]\n",
    "#     # Make sure these farmers are part of the routes\n",
    "#     farmers_inroute = [n for node in [route['route'] for key, routes in data['routes'].iteritems() if len(routes)>0 for route in routes] for n in node]\n",
    "#     if (set(farmers_inroute)!=set(farmers)):\n",
    "#         print(\"error\")\n",
    "#         print(farmers_inroute)\n",
    "#         print(farmers)\n",
    "#         print(name_instance)\n",
    "        \n",
    "#     # Get upper and lower bounds\n",
    "#     z_ub += data['z_ub']\n",
    "#     z_lb += data['z_lb']\n",
    "    \n",
    "#     # Fix farmers that are picked up more than once\n",
    "#     remove_repetitions(data,N,vrp)\n",
    "    \n",
    "#     instances_parsed.append({\n",
    "#         \"n_nodes\": len([n for n in data['mapping'].keys() if n[0]=='n']),\n",
    "#         \"cluster\": cluster,\n",
    "#         \"n_trucks\": len([n for n in data['mapping'].keys() if n[0]=='h']),\n",
    "#         \"z_ub\":data['z_ub'],\n",
    "#         \"z_lb\":data['z_lb'],\n",
    "#         \"gap\":(data['z_ub']-data['z_lb'])/data['z_ub'],\n",
    "#         \"nodes_served\":[data['mapping'][n] for n in data['mapping'].keys() if n[0]=='n']\n",
    "#     })\n",
    "    \n",
    "#     for t in data['routes'].keys():\n",
    "#         for route in data['routes'][t]:\n",
    "#             detailed_dist = path_distance([t]+route['route']+[t],mapping,'SKIP')\n",
    "#             real_route = [data['mapping'][t][0]]+[data['mapping'][n] for n in route[\"route\"] if n[0]=='n']+['SKIP']+[data['mapping'][t][0]]\n",
    "#             truck_loads.append({\n",
    "#                     \"type\":\"temporal\",\n",
    "#                     \"cluster\":int(cluster),\n",
    "#                     \"truck\":data['mapping'][t][0],\n",
    "#                     \"day\":-1,\n",
    "#                     \"max_load\":route[\"max_load\"],\n",
    "#                     \"load\":np.sum([quantities[n] for n in route['route']]),\n",
    "#                     \"load_perc\":np.sum([quantities[n] for n in route['route']])/(route[\"max_load\"]+0.0),\n",
    "#                     \"n_farmers\":len(set(route[\"route\"])),\n",
    "#                     \"distance\":vrp.distance_path([t]+route['route']+[t]),\n",
    "#                     \"detailed_distance\":detailed_dist['distance'],\n",
    "#                     \"gravel\":detailed_dist['detailed_distance']['gravel'],\n",
    "#                     \"asphalt\":detailed_dist['detailed_distance']['asphalt'],\n",
    "#                     \"access\":detailed_dist['detailed_distance']['access'],\n",
    "#                     \"dirt\":detailed_dist['detailed_distance']['dirt'],\n",
    "#                     \"unknown\":detailed_dist['detailed_distance']['unknown'],\n",
    "#                     \"road_time\":detailed_dist['time'],\n",
    "#                     \"farmers\":[data['mapping'][n] for n in route[\"route\"] if n[0]=='n'],\n",
    "#                     \"route\":real_route,\n",
    "#                     # This contains the node and its location\n",
    "#                     \"waypoints\":[(r,positions_dict[r]) for r in real_route],\n",
    "#                     \"retained_farmers\":np.sum(np.array([original_middleman[data['mapping'][n]] for n in route[\"route\"] if n[0]=='n'] ) == int(cluster))\n",
    "#             })\n",
    "\n",
    "# print (z_ub)\n",
    "# print (z_lb)\n",
    "# print((z_ub-z_lb)/z_ub)\n",
    "# temporal_df = pd.DataFrame(instances_parsed)\n",
    "# temporal_df.to_csv(results_folder+(\"tables_for_report/runs_temporal.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract polygons\n",
    "for r in truck_loads:\n",
    "    r[\"polygon\"] = []\n",
    "    for i in range(len(r[\"waypoints\"])-1):\n",
    "        r[\"polygon\"] = r[\"polygon\"] + [road_distance.waypoints(r[\"waypoints\"][i][1],r[\"waypoints\"][i+1][1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_folder_ts = 'Results/'+ ts + \"/\" \n",
    "\n",
    "# Save polygons in a JSON file\n",
    "with open(results_folder_ts + 'parsed_routes.json', 'w') as fp:\n",
    "    json.dump(truck_loads,fp, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results/Jan142018/'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_folder_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
