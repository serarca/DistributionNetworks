{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import math\n",
    "from dateutil.parser import parse\n",
    "from geopandas import GeoDataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import json\n",
    "import ast\n",
    "import sys\n",
    "import pickle\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "os.chdir(\"/Users/sergiocamelo/Dropbox/Sergio-Joann/\")\n",
    "\n",
    "sys.path.insert(0, 'Code/VRPEngine/pyCode')\n",
    "sys.path.insert(0, 'Code/VRPEngine/C++Engine')\n",
    "sys.path.insert(0, 'Code/VRPEngine/pyCode/tsp')\n",
    "\n",
    "import solver as solver\n",
    "import distances as distances\n",
    "import VRPClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_folder = 'Results'\n",
    "data_folder = results_folder+st+\"/\"\n",
    "\n",
    "# The folder which will be used to write the bash scripts\n",
    "folder_project = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters for the run\n",
    "season = \"high\"\n",
    "period = 3*28 # Use three month period\n",
    "days_analysis = 14\n",
    "st = 'Jan132018'\n",
    "farmers_data = 'dim_farmers_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 17] File exists: 'Results/Jan132018/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-eee12c44cc46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create random timestamped folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 17] File exists: 'Results/Jan132018/'"
     ]
    }
   ],
   "source": [
    "# Create random timestamped folder\n",
    "os.mkdir(results_folder+\"_\"+st+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already created\n"
     ]
    }
   ],
   "source": [
    "folder_data_cleaning_results = 'data_cleaning_results' + st\n",
    "try:\n",
    "    os.mkdir(results_folder+\"_\"+st+\"/\"+folder_data_cleaning_results+'/')\n",
    "except:\n",
    "    print(\"Folder already created\")\n",
    "f = open('StandardizedData/%s/report.txt'%(folder_data_cleaning_results), 'w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open(data_folder + '/errors.txt', 'a').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_volume = season + '_volume'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "dim_farmers = pd.read_csv(data_folder+farmers_data)\n",
    "dim_middlemen = pd.read_csv(data_folder+'dim_middlemen.csv')\n",
    "dim_mills = pd.read_csv(data_folder+'dim_mills.csv')\n",
    "harvest_frequency_mapping = pd.read_csv(data_folder+'harvest_frequency_mapping.csv')\n",
    "print(\"Number of plantations: %d\" % (len(dim_farmers)),file=f)\n",
    "print(\"Number of middlemen: %d\" % (len(dim_middlemen)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a unique identifier for the farmer plot\n",
    "dim_farmers['plot_id'] = [dim_farmers['farmer_id'][i] + '-'+str(dim_farmers['plot_number'][i]) for i in range(len(dim_farmers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 repetitions\n"
     ]
    }
   ],
   "source": [
    "# Delete repetitions\n",
    "old_len = len(dim_farmers)\n",
    "dim_farmers = copy.deepcopy(dim_farmers[-dim_farmers['plot_id'].duplicated()]).reset_index(drop=True)\n",
    "print(\"Found %d repetitions\"%(old_len-len(dim_farmers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of the farmers to cluster\n",
    "farmer_to_cluster = dict(zip(dim_farmers['plot_id'],dim_farmers['cluster_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate middleman capacity\n",
    "dim_middlemen['trucks_dict']  = dim_middlemen['trucks'].map(lambda d:ast.literal_eval(d))\n",
    "dim_middlemen['capacity'] = dim_middlemen['trucks_dict'].map(lambda d:np.sum([int(t)*d[t] for t in d.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join farmers and middlemen data\n",
    "# Change names of columns\n",
    "dim_farmers = dim_farmers.rename(index=str, columns={\"latitude\": \"latitude_farmer\", \"longitude\": \"longitude_farmer\"})\n",
    "dim_middlemen = dim_middlemen.rename(index=str, columns={\"latitude\": \"latitude_middleman\", \"longitude\": \"longitude_middleman\"})\n",
    "dim_mills = dim_mills.rename(index=str, columns={\"latitude\": \"latitude_mill\", \"longitude\": \"longitude_mill\"})\n",
    "\n",
    "result = pd.merge(dim_farmers, dim_middlemen, on=['cluster_id'], how='inner')\n",
    "print(\"Number of plantations with middleman: %d\" % (len(result)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if there are any duplicates\n",
    "result[result.duplicated(subset=['farmer_id','plot_number'], keep=False)].to_csv(data_folder+folder_data_cleaning_results+'/duplicates.csv')\n",
    "print(\"Total of duplicates: %d\" % (len(result[result.duplicated(subset=['farmer_id','plot_number'], keep=False)])),file=f)\n",
    "print (\"Duplicates saved in data_cleaning_results/duplicates.csv\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use only data with lat_lon and with productions\n",
    "df_full = result[np.logical_and(pd.notnull(result['longitude_farmer']),pd.notnull(result['latitude_farmer']))].copy()\n",
    "print(\"Number of plantations with latlon: %d\" % (len(df_full)),file=f)\n",
    "# Use data with productions\n",
    "df_full = df_full[df_full[season+'_rate']!=0].copy()\n",
    "df_full = df_full[pd.notnull(df_full[season+'_rate'])].copy()\n",
    "df_full = df_full[df_full[season+'_volume']!=0].copy()\n",
    "print(\"Number of plantations that produce during the season : %d\" % (len(df_full)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map number of days\n",
    "harvest_frequency_mapping_dict = {row[0]:row[1] for i,row in harvest_frequency_mapping.iterrows()}\n",
    "df_full['rate'] = df_full[season+'_rate'].map(harvest_frequency_mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Has pickup date \n",
    "df_full = df_full[pd.notnull(df_full['date_last_sold'])].copy()\n",
    "print(\"Number of plantations with last date: %d\" % (len(df_full)),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate days of pickup\n",
    "ref_day = datetime.datetime.strptime('1/3/2000', \"%m/%d/%Y\")\n",
    "days = np.array([(parse(v)-ref_day).days for v in df_full['date_last_sold'].values])\n",
    "df_full['day_mod'] = days%period\n",
    "def calculate_pickup_days(row):\n",
    "    d = row['day_mod']\n",
    "    freq = row['rate']\n",
    "    l = []\n",
    "    for i in range(int(period/freq)):\n",
    "        l.append((d + i * freq)%period)\n",
    "    return l\n",
    "df_full['pickup_days'] = df_full.apply(calculate_pickup_days, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explote data\n",
    "clusters = np.unique(df_full['cluster_id'])\n",
    "df_exploted = pd.merge(df_full,df_full.pickup_days.apply(pd.Series).stack().reset_index(level=1, drop=True).to_frame('pickup'),left_index=True, right_index=True)\n",
    "for c in clusters:\n",
    "    print(\"Number of plantations of cluster %d: %d\" % (c,len(df_full[df_full['cluster_id'] == c].copy())),file=f)\n",
    "df_clusters = df_exploted[(np.array([c in clusters for c in df_exploted['cluster_id']])) & (df_exploted['pickup'] < days_analysis)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print middlemen data in the results folder\n",
    "dim_middlemen[[dim_middlemen['cluster_id'][i] in clusters for i in range(len(dim_middlemen))]][['cluster_id','trucks','mills','capacity']].to_csv(data_folder+folder_data_cleaning_results+\"/middlemen_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate total capacity\n",
    "dict_comparisons={}\n",
    "for c in clusters:\n",
    "    dict_comparisons[c] = {}\n",
    "    dict_comparisons[c]['capacity'] = dim_middlemen[dim_middlemen.cluster_id == c]['capacity'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Round producing quantities to the decimal up\n",
    "df_clusters[season + '_volume'] = np.ceil(df_clusters[season + '_volume'] * 10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   farmer_id  high_volume  overload\n",
      "cluster_id pickup                                  \n",
      "13.0       0.0            13         23.6       3.6\n",
      "           1.0             5         20.4       0.4\n",
      "           3.0            10         21.6       1.6\n",
      "           13.0            7         21.4       1.4\n",
      "41.0       6.0             4         14.7       4.7\n",
      "46.0       2.0             6         23.4      12.4\n",
      "           6.0             8         26.1      15.1\n",
      "           9.0             8         32.7      21.7\n",
      "51.0       9.0             5         10.3       1.3\n",
      "           13.0            5         13.1       4.1\n",
      "60.0       2.0             8         18.6       0.6\n",
      "           3.0             6         23.7       5.7\n",
      "           4.0            11         20.4       2.4\n",
      "           7.0             9         25.8       7.8\n",
      "           8.0             8         41.5      23.5\n",
      "108.0      2.0            13         21.2       3.2\n",
      "199.0      5.0             3         14.0       3.0\n",
      "241.0      1.0             2          2.1       0.1\n",
      "           4.0             2          2.6       0.6\n",
      "           5.0             4          5.0       3.0\n",
      "           9.0             2          2.5       0.5\n",
      "           11.0            2          4.2       2.2\n",
      "           13.0            2          3.3       1.3\n",
      "243.0      6.0             8         25.2      14.2\n",
      "258.0      10.0            1         20.0       9.0\n",
      "366.0      0.0             5         12.6       3.6\n",
      "           1.0             4         10.3       1.3\n",
      "           2.0             5         13.5       4.5\n",
      "           5.0             8         22.8      13.8\n",
      "           6.0             8         15.5       6.5\n",
      "           7.0             9         24.2      15.2\n",
      "           8.0             6         10.0       1.0\n",
      "           9.0             8         21.6      12.6\n",
      "           12.0            7         17.7       8.7\n",
      "591.0      7.0             7         12.9       3.9\n",
      "           9.0             4          9.8       0.8\n"
     ]
    }
   ],
   "source": [
    "# Number of plantations picked up each day and quantities picked up each day\n",
    "# Calculate the number of days\n",
    "agg_quant = df_clusters.groupby(['cluster_id','pickup']).agg({'farmer_id':'count', season+'_volume': 'sum'})\n",
    "agg_quant['overload']=agg_quant[season+'_volume']-agg_quant.apply(lambda r:dict_comparisons[r.name[0]]['capacity'],1)\n",
    "outliers = (agg_quant[agg_quant['overload']>0])\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a report of inconsistent data\n",
    "report = []\n",
    "for index,row in outliers.iterrows():\n",
    "    report.append({'cluster':index[0],\n",
    "                   'farmers':row['farmer_id'],\n",
    "                         season+'_volume':row[season+'_volume'],\n",
    "                         'capacity':row[season+'_volume']-row['overload'],\n",
    "                        'farmer_id-plot':[r['farmer_id']+'-'+str(r['plot_number']) for j,r in df_clusters.iterrows() if (r['cluster_id']==index[0] and r['pickup']==index[1])]})\n",
    "print(\"Found %d trucks carrying more than their capacity\" % len(outliers),file=f)\n",
    "pd.DataFrame(report).to_csv(data_folder+folder_data_cleaning_results+'/overcapacity.csv')\n",
    "print(\"Report saved in data_cleaning_results/overcapacity.csv\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a CSV that stores the plot id, the quantity, the rate and the pickup-days\n",
    "summarized_dict = collections.defaultdict(dict)\n",
    "for i,row in df_clusters.iterrows():\n",
    "    if 'farmer_id' in summarized_dict[row['plot_id']]:\n",
    "        assert(summarized_dict[row['plot_id']]['farmer_id'] == row['farmer_id'])\n",
    "    else:\n",
    "        summarized_dict[row['plot_id']]['farmer_id'] = row['farmer_id']\n",
    "\n",
    "    if 'rate' in summarized_dict[row['plot_id']]:\n",
    "        assert(summarized_dict[row['plot_id']]['rate'] == row['rate'])\n",
    "    else:\n",
    "        summarized_dict[row['plot_id']]['rate'] = row['rate']\n",
    "        \n",
    "    if 'volume' in summarized_dict[row['plot_id']]:\n",
    "        assert(summarized_dict[row['plot_id']]['volume'] == row[season+\"_volume\"])\n",
    "    else:\n",
    "        summarized_dict[row['plot_id']]['volume'] = row[season+\"_volume\"]\n",
    "        \n",
    "    if 'cluster_id' in summarized_dict[row['plot_id']]:\n",
    "        if (row['farmer_id'] not in ['F14020080487','F14020080660','F14020080219','F14020080253']):\n",
    "            assert(summarized_dict[row['plot_id']]['cluster_id'] == row['cluster_id'])\n",
    "    else:\n",
    "        summarized_dict[row['plot_id']]['cluster_id'] = row['cluster_id']\n",
    "        \n",
    "    summarized_dict[row['plot_id']]['latitude'] = row['latitude_farmer']\n",
    "    summarized_dict[row['plot_id']]['longitude'] = row['longitude_farmer']\n",
    "    \n",
    "    if 'pickups' not in summarized_dict[row['plot_id']]:\n",
    "        summarized_dict[row['plot_id']]['pickups'] = [row['pickup']]\n",
    "    else:\n",
    "        summarized_dict[row['plot_id']]['pickups'] = summarized_dict[row['plot_id']]['pickups'] + [row['pickup']]\n",
    "        \n",
    "pd.DataFrame(summarized_dict).transpose().to_csv(data_folder+folder_data_cleaning_results+'/dim_pickups.csv',index_label = \"plot_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create trucks dataset (Only run once)\n",
    "truck_dicts = []\n",
    "j = 0\n",
    "for i,r in dim_middlemen.iterrows():\n",
    "    truck_dic = ast.literal_eval(r['trucks'])\n",
    "    for capacity in truck_dic.keys():\n",
    "        for i in range(truck_dic[capacity]):\n",
    "            truck_dicts.append({\n",
    "                    \"cluster_id\":r['cluster_id'],\n",
    "                    \"truck_id\":\"t_\"+str(j),\n",
    "                    \"capacity\":int(capacity)\n",
    "                })\n",
    "            j += 1\n",
    "pd.DataFrame(truck_dicts).to_csv(data_folder+folder_data_cleaning_results+\"/dim_trucks.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the trucks dataset\n",
    "dim_trucks = pd.read_csv(data_folder+folder_data_cleaning_results+\"/dim_trucks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataset with code to position\n",
    "positions_dict = {}\n",
    "for i, row in dim_mills.iterrows():\n",
    "    positions_dict[row[\"code\"]] = (row['latitude_mill'], row['longitude_mill'])\n",
    "for i, row in dim_farmers.iterrows():\n",
    "    positions_dict[row[\"farmer_id\"]+'-'+str(row[\"plot_number\"])] = (row['latitude_farmer'], row['longitude_farmer'])\n",
    "for i, row in dim_middlemen.iterrows():\n",
    "    positions_dict[row[\"cluster_id\"]] = (row['latitude_middleman'], row['longitude_middleman'])\n",
    "for i, row in dim_trucks.iterrows():\n",
    "    positions_dict[row[\"truck_id\"]] = positions_dict[float(row[\"cluster_id\"])]\n",
    "unique_id_to_latlon = pd.DataFrame(positions_dict).transpose()\n",
    "unique_id_to_latlon.columns = ['latitude', 'longitude']\n",
    "unique_id_to_latlon.to_csv(data_folder+folder_data_cleaning_results+\"/unique_id_to_latlon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
